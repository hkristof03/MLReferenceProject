training_arguments:
  num_train_epochs: 3
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  weight_decay: 0.01
  dataloader_num_workers: 8
  fp16: True
  logging_steps: 500
  evaluation_strategy: 'epoch'
  save_strategy: 'epoch'
  load_best_model_at_end: True
  metric_for_best_model: 'eval_precision'
  save_total_limit: 1

optimizer:
  name: AdamW
  params:
    lr: 1e-2

schedule:
  name: get_linear_schedule_with_warmup
  params:
    num_warmup_steps: 100

save_model: True